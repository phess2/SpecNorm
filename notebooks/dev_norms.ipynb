{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from argparse import ArgumentParser\n",
    "import yaml\n",
    "import json\n",
    "import socket\n",
    "import os\n",
    "\n",
    "from importlib import reload\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from src.classifiers import *\n",
    "from corpus.CIFAR100_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reload(\u001b[43msrc\u001b[49m\u001b[38;5;241m.\u001b[39mclassifiers)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'src' is not defined"
     ]
    }
   ],
   "source": [
    "reload(src.classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/weka/scratch/weka/mcdermott/rphess/conda_envs/pytorch_2_tv/lib/python3.11/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' and 'progress' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/weka/scratch/weka/mcdermott/rphess/conda_envs/pytorch_2_tv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = CIFAR100_Resnet('resnet18', 'frob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 1, 1])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([256, 64, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 64, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([64, 256, 1, 1])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([256, 64, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([64, 256, 1, 1])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([256, 64, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([128, 256, 1, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([512, 128, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 256, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([128, 512, 1, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([512, 128, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([128, 512, 1, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([512, 128, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([128, 512, 1, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([512, 128, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([256, 512, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([1024, 256, 1, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 512, 1, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([256, 1024, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([1024, 256, 1, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([256, 1024, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([1024, 256, 1, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([256, 1024, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([1024, 256, 1, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([256, 1024, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([1024, 256, 1, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([256, 1024, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([1024, 256, 1, 1])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([512, 1024, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512, 1, 1])\n",
      "torch.Size([2048])\n",
      "torch.Size([2048])\n",
      "torch.Size([2048, 1024, 1, 1])\n",
      "torch.Size([2048])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512, 1, 1])\n",
      "torch.Size([2048])\n",
      "torch.Size([2048])\n",
      "torch.Size([512, 2048, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 512, 1, 1])\n",
      "torch.Size([2048])\n",
      "torch.Size([2048])\n",
      "torch.Size([100, 512])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight\n",
      "model.bn1.weight\n",
      "model.bn1.bias\n",
      "model.layer1.0.conv1.weight\n",
      "model.layer1.0.bn1.weight\n",
      "model.layer1.0.bn1.bias\n",
      "model.layer1.0.conv2.weight\n",
      "model.layer1.0.bn2.weight\n",
      "model.layer1.0.bn2.bias\n",
      "model.layer1.0.conv3.weight\n",
      "model.layer1.0.bn3.weight\n",
      "model.layer1.0.bn3.bias\n",
      "model.layer1.0.downsample.0.weight\n",
      "model.layer1.0.downsample.1.weight\n",
      "model.layer1.0.downsample.1.bias\n",
      "model.layer1.1.conv1.weight\n",
      "model.layer1.1.bn1.weight\n",
      "model.layer1.1.bn1.bias\n",
      "model.layer1.1.conv2.weight\n",
      "model.layer1.1.bn2.weight\n",
      "model.layer1.1.bn2.bias\n",
      "model.layer1.1.conv3.weight\n",
      "model.layer1.1.bn3.weight\n",
      "model.layer1.1.bn3.bias\n",
      "model.layer1.2.conv1.weight\n",
      "model.layer1.2.bn1.weight\n",
      "model.layer1.2.bn1.bias\n",
      "model.layer1.2.conv2.weight\n",
      "model.layer1.2.bn2.weight\n",
      "model.layer1.2.bn2.bias\n",
      "model.layer1.2.conv3.weight\n",
      "model.layer1.2.bn3.weight\n",
      "model.layer1.2.bn3.bias\n",
      "model.layer2.0.conv1.weight\n",
      "model.layer2.0.bn1.weight\n",
      "model.layer2.0.bn1.bias\n",
      "model.layer2.0.conv2.weight\n",
      "model.layer2.0.bn2.weight\n",
      "model.layer2.0.bn2.bias\n",
      "model.layer2.0.conv3.weight\n",
      "model.layer2.0.bn3.weight\n",
      "model.layer2.0.bn3.bias\n",
      "model.layer2.0.downsample.0.weight\n",
      "model.layer2.0.downsample.1.weight\n",
      "model.layer2.0.downsample.1.bias\n",
      "model.layer2.1.conv1.weight\n",
      "model.layer2.1.bn1.weight\n",
      "model.layer2.1.bn1.bias\n",
      "model.layer2.1.conv2.weight\n",
      "model.layer2.1.bn2.weight\n",
      "model.layer2.1.bn2.bias\n",
      "model.layer2.1.conv3.weight\n",
      "model.layer2.1.bn3.weight\n",
      "model.layer2.1.bn3.bias\n",
      "model.layer2.2.conv1.weight\n",
      "model.layer2.2.bn1.weight\n",
      "model.layer2.2.bn1.bias\n",
      "model.layer2.2.conv2.weight\n",
      "model.layer2.2.bn2.weight\n",
      "model.layer2.2.bn2.bias\n",
      "model.layer2.2.conv3.weight\n",
      "model.layer2.2.bn3.weight\n",
      "model.layer2.2.bn3.bias\n",
      "model.layer2.3.conv1.weight\n",
      "model.layer2.3.bn1.weight\n",
      "model.layer2.3.bn1.bias\n",
      "model.layer2.3.conv2.weight\n",
      "model.layer2.3.bn2.weight\n",
      "model.layer2.3.bn2.bias\n",
      "model.layer2.3.conv3.weight\n",
      "model.layer2.3.bn3.weight\n",
      "model.layer2.3.bn3.bias\n",
      "model.layer3.0.conv1.weight\n",
      "model.layer3.0.bn1.weight\n",
      "model.layer3.0.bn1.bias\n",
      "model.layer3.0.conv2.weight\n",
      "model.layer3.0.bn2.weight\n",
      "model.layer3.0.bn2.bias\n",
      "model.layer3.0.conv3.weight\n",
      "model.layer3.0.bn3.weight\n",
      "model.layer3.0.bn3.bias\n",
      "model.layer3.0.downsample.0.weight\n",
      "model.layer3.0.downsample.1.weight\n",
      "model.layer3.0.downsample.1.bias\n",
      "model.layer3.1.conv1.weight\n",
      "model.layer3.1.bn1.weight\n",
      "model.layer3.1.bn1.bias\n",
      "model.layer3.1.conv2.weight\n",
      "model.layer3.1.bn2.weight\n",
      "model.layer3.1.bn2.bias\n",
      "model.layer3.1.conv3.weight\n",
      "model.layer3.1.bn3.weight\n",
      "model.layer3.1.bn3.bias\n",
      "model.layer3.2.conv1.weight\n",
      "model.layer3.2.bn1.weight\n",
      "model.layer3.2.bn1.bias\n",
      "model.layer3.2.conv2.weight\n",
      "model.layer3.2.bn2.weight\n",
      "model.layer3.2.bn2.bias\n",
      "model.layer3.2.conv3.weight\n",
      "model.layer3.2.bn3.weight\n",
      "model.layer3.2.bn3.bias\n",
      "model.layer3.3.conv1.weight\n",
      "model.layer3.3.bn1.weight\n",
      "model.layer3.3.bn1.bias\n",
      "model.layer3.3.conv2.weight\n",
      "model.layer3.3.bn2.weight\n",
      "model.layer3.3.bn2.bias\n",
      "model.layer3.3.conv3.weight\n",
      "model.layer3.3.bn3.weight\n",
      "model.layer3.3.bn3.bias\n",
      "model.layer3.4.conv1.weight\n",
      "model.layer3.4.bn1.weight\n",
      "model.layer3.4.bn1.bias\n",
      "model.layer3.4.conv2.weight\n",
      "model.layer3.4.bn2.weight\n",
      "model.layer3.4.bn2.bias\n",
      "model.layer3.4.conv3.weight\n",
      "model.layer3.4.bn3.weight\n",
      "model.layer3.4.bn3.bias\n",
      "model.layer3.5.conv1.weight\n",
      "model.layer3.5.bn1.weight\n",
      "model.layer3.5.bn1.bias\n",
      "model.layer3.5.conv2.weight\n",
      "model.layer3.5.bn2.weight\n",
      "model.layer3.5.bn2.bias\n",
      "model.layer3.5.conv3.weight\n",
      "model.layer3.5.bn3.weight\n",
      "model.layer3.5.bn3.bias\n",
      "model.layer4.0.conv1.weight\n",
      "model.layer4.0.bn1.weight\n",
      "model.layer4.0.bn1.bias\n",
      "model.layer4.0.conv2.weight\n",
      "model.layer4.0.bn2.weight\n",
      "model.layer4.0.bn2.bias\n",
      "model.layer4.0.conv3.weight\n",
      "model.layer4.0.bn3.weight\n",
      "model.layer4.0.bn3.bias\n",
      "model.layer4.0.downsample.0.weight\n",
      "model.layer4.0.downsample.1.weight\n",
      "model.layer4.0.downsample.1.bias\n",
      "model.layer4.1.conv1.weight\n",
      "model.layer4.1.bn1.weight\n",
      "model.layer4.1.bn1.bias\n",
      "model.layer4.1.conv2.weight\n",
      "model.layer4.1.bn2.weight\n",
      "model.layer4.1.bn2.bias\n",
      "model.layer4.1.conv3.weight\n",
      "model.layer4.1.bn3.weight\n",
      "model.layer4.1.bn3.bias\n",
      "model.layer4.2.conv1.weight\n",
      "model.layer4.2.bn1.weight\n",
      "model.layer4.2.bn1.bias\n",
      "model.layer4.2.conv2.weight\n",
      "model.layer4.2.bn2.weight\n",
      "model.layer4.2.bn2.bias\n",
      "model.layer4.2.conv3.weight\n",
      "model.layer4.2.bn3.weight\n",
      "model.layer4.2.bn3.bias\n",
      "model.fc.weight\n",
      "model.fc.bias\n"
     ]
    }
   ],
   "source": [
    "target_norms = dict()\n",
    "for idx, data in enumerate(model.named_parameters()):\n",
    "    name, param = data\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "        if 'bn' in name or 'bias' in name:\n",
    "            continue\n",
    "        else:\n",
    "            frob_norm = torch.norm(param, p='fro')\n",
    "            target_norms[idx] = frob_norm.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3370358677.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[37], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    model.r{name}\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4.585747241973877,\n",
       " 3: 4.541250228881836,\n",
       " 6: 5.59863805770874,\n",
       " 9: 4.514338493347168,\n",
       " 12: 7.373269557952881,\n",
       " 13: 3.82240629196167,\n",
       " 15: 3.9235951900482178,\n",
       " 18: 5.531525611877441,\n",
       " 21: 4.165121078491211,\n",
       " 24: 3.850189447402954,\n",
       " 27: 6.102963924407959,\n",
       " 30: 3.9922902584075928,\n",
       " 33: 6.417440414428711,\n",
       " 36: 8.56154727935791,\n",
       " 39: 7.189218521118164,\n",
       " 42: 8.3694486618042,\n",
       " 43: 3.4934608936309814,\n",
       " 45: 4.264687538146973,\n",
       " 48: 7.4251813888549805,\n",
       " 51: 5.655665397644043,\n",
       " 54: 5.972181797027588,\n",
       " 57: 8.293109893798828,\n",
       " 60: 6.701646327972412,\n",
       " 63: 6.230434894561768,\n",
       " 66: 8.60983657836914,\n",
       " 69: 6.232676982879639,\n",
       " 72: 11.14791202545166,\n",
       " 75: 13.338133811950684,\n",
       " 78: 11.969951629638672,\n",
       " 81: 11.69320297241211,\n",
       " 82: 3.9300601482391357,\n",
       " 84: 7.794566631317139,\n",
       " 87: 11.597563743591309,\n",
       " 90: 10.43730354309082,\n",
       " 93: 8.068238258361816,\n",
       " 96: 11.535717010498047,\n",
       " 99: 9.775774002075195,\n",
       " 102: 8.8884859085083,\n",
       " 105: 11.477972984313965,\n",
       " 108: 9.342968940734863,\n",
       " 111: 9.287092208862305,\n",
       " 114: 11.444696426391602,\n",
       " 117: 9.33027458190918,\n",
       " 120: 10.100244522094727,\n",
       " 123: 11.725800514221191,\n",
       " 126: 9.9190034866333,\n",
       " 129: 17.2592830657959,\n",
       " 132: 19.2991943359375,\n",
       " 135: 15.67155933380127,\n",
       " 138: 14.387815475463867,\n",
       " 139: 12.560022354125977,\n",
       " 141: 15.104610443115234,\n",
       " 144: 18.928661346435547,\n",
       " 147: 15.31066608428955,\n",
       " 150: 18.57474708557129,\n",
       " 153: 16.99479866027832,\n",
       " 156: 14.514214515686035,\n",
       " 159: 5.7727861404418945}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(4.5857, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(8., grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(0., grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(4.5413, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.6665, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.2151, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(5.5986, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.3491, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.6430, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(4.5143, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.6099, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.2045, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(7.3733, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(3.8224, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.2045, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(3.9236, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.5695, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.1827, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(5.5315, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.5004, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.0557, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(4.1651, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.6445, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(0.7951, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(3.8502, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.4788, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(0.7833, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(6.1030, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.7868, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(0.6971, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(3.9923, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.8395, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(0.9510, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(6.4174, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.3671, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.3107, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(8.5615, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.3078, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.0611, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(7.1892, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(3.1005, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.1529, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(8.3694, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(3.4935, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.1529, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(4.2647, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.3616, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.2483, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(7.4252, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.8928, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(0.9171, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(5.6557, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.8049, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.3091, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(5.9722, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.9667, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(0.8249, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(8.2931, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.0964, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(0.9559, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(6.7016, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.6752, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.8093, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(6.2304, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.9450, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(0.8305, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(8.6098, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.3180, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.1257, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(6.2327, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.4594, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.9441, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(11.1479, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(3.7729, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.4451, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(13.3381, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.9443, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.4086, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(11.9700, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(4.8335, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.1845, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(11.6932, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(3.9301, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.1845, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(7.7946, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.5530, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(0.9458, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(11.5976, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(3.0476, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.5117, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(10.4373, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(3.5868, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.4226, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(8.0682, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.5229, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.2148, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(11.5357, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.9852, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.5966, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(9.7758, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(3.1635, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.3784, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(8.8885, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.6756, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.6742, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(11.4780, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.7941, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.4584, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(9.3430, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(3.1865, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.5054, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(9.2871, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.7129, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.8254, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(11.4447, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.8771, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.7564, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(9.3303, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(3.3109, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(3.1934, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(10.1002, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.9879, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.0737, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(11.7258, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(3.1514, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.7560, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(9.9190, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(3.7455, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(4.1625, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(17.2593, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(5.0234, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(4.2364, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(19.2992, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(4.4521, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.8652, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(15.6716, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(16.6505, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.7417, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(14.3878, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(12.5600, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.7417, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(15.1046, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(4.4145, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(3.0358, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(18.9287, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(4.7773, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.9050, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(15.3107, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(16.6097, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(3.3519, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(18.5747, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(4.9470, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(4.1405, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(16.9948, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(4.8269, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(2.2553, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(14.5142, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(33.1176, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(1.6063, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(5.7728, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(0.2564, grad_fn=<LinalgVectorNormBackward0>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.target_norms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
